{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUlW1U2iDhZa5268f2ywKg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munnurumahesh03-coder/machine-learning-for-classification/blob/main/Capstone_Project_For_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stacking Classifier (3 Models)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PcijYL5qcKgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz__UxNtZ-PB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data = pd.read_csv('australia.csv')"
      ],
      "metadata": {
        "id": "RPY_LYfSayvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data"
      ],
      "metadata": {
        "id": "zwZdrhXYayr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TZxhZp5ZeC0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.info()"
      ],
      "metadata": {
        "id": "34j5-6gYayo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.describe()"
      ],
      "metadata": {
        "id": "wf8REY8YaykW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.isnull().sum()"
      ],
      "metadata": {
        "id": "KYaLAAt_ayhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original number of rows: {len(weather_data)}\")\n",
        "weather_data.dropna(subset=['RainTomorrow'], inplace=True)\n",
        "print(f\"Number of rows after dropping missing target values: {len(weather_data)}\")"
      ],
      "metadata": {
        "id": "8nhYjnOsayd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDistribution of 'RainTomorrow':\")\n",
        "print(weather_data['RainTomorrow'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "xZgUN2d9aybL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConverting target variable to numerical format (0 for 'No', 1 for 'Yes')...\")\n",
        "weather_data['RainTomorrow'] = weather_data['RainTomorrow'].map({'No': 0, 'Yes': 1})\n",
        "print(\"Conversion complete.\")\n",
        "print(\"First 5 values of the transformed 'RainTomorrow' column:\")\n",
        "print(weather_data['RainTomorrow'].head())"
      ],
      "metadata": {
        "id": "m_Oq9IGNayYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.info()"
      ],
      "metadata": {
        "id": "jjhJD6W8ayVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data.describe()"
      ],
      "metadata": {
        "id": "dSOCTUygaySb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fsQ5tSUleagZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "sns.countplot(\n",
        "    data=weather_data,\n",
        "    x='RainTomorrow',\n",
        "    ax=ax,\n",
        "    palette='viridis'\n",
        ")\n",
        "ax.set_title('Distribution of Target Variable: RainTomorrow', fontsize=16)\n",
        "ax.set_xlabel('Will it Rain Tomorrow?', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)"
      ],
      "metadata": {
        "id": "gTODYJbMayPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "\n",
        "sns.histplot(\n",
        "    data=weather_data,\n",
        "    x='Rainfall',\n",
        "    ax=ax,\n",
        "    bins=50\n",
        ")\n",
        "\n",
        "ax.set_yscale('log')\n",
        "\n",
        "ax.set_title('Distribution of Rainfall (Log Scale)', fontsize=16)\n",
        "ax.set_xlabel('Rainfall (mm)', fontsize=12)\n",
        "ax.set_ylabel('Frequency (Log Scale)', fontsize=12)"
      ],
      "metadata": {
        "id": "2MEjsFfBayL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(\n",
        "    data=weather_data,\n",
        "    x='RainTomorrow',\n",
        "    y='Humidity3pm',\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "plt.title('Humidity at 3pm vs. Rain Tomorrow', fontsize=16)\n",
        "plt.xlabel('Did it Rain Tomorrow?', fontsize=12)\n",
        "plt.ylabel('Humidity at 3pm (%)', fontsize=12)"
      ],
      "metadata": {
        "id": "dUlFvEtnayI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.boxplot(\n",
        "    data=weather_data,\n",
        "    x='RainTomorrow',\n",
        "    y='Sunshine',\n",
        "    palette='plasma'\n",
        ")\n",
        "\n",
        "plt.title('Hours of Sunshine vs. Rain Tomorrow', fontsize=16)\n",
        "plt.xlabel('Did it Rain Tomorrow?', fontsize=12)\n",
        "plt.ylabel('Sunshine (Hours)', fontsize=12)"
      ],
      "metadata": {
        "id": "m6ubq1gnayF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 12)) # Create a new, separate figure for the large heatmap\n",
        "correlation_matrix = weather_data.select_dtypes(include=['float64', 'int64']).corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.1f', linewidths=.5)\n",
        "plt.title('Correlation Heatmap of All Numerical Features', fontsize=18)"
      ],
      "metadata": {
        "id": "3kUVeCj5ayCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VKo4YjdTetw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Make a copy to avoid changing the original raw data ---\n",
        "df_eng = weather_data.copy()\n",
        "\n",
        "# --- 1. Date-Based Features ---\n",
        "# First, ensure the 'Date' column is in datetime format\n",
        "df_eng['Date'] = pd.to_datetime(df_eng['Date'])\n",
        "\n",
        "# Extract year, month, and day\n",
        "df_eng['Year'] = df_eng['Date'].dt.year\n",
        "df_eng['Month'] = df_eng['Date'].dt.month\n",
        "df_eng['Day'] = df_eng['Date'].dt.day\n",
        "\n",
        "# It's often useful to know the day of the year as well\n",
        "df_eng['DayOfYear'] = df_eng['Date'].dt.dayofyear\n",
        "\n",
        "print(\"Created date-based features: Year, Month, Day, DayOfYear\")"
      ],
      "metadata": {
        "id": "ylWzq2LUayAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily temperature range\n",
        "df_eng['TempRange'] = df_eng['MaxTemp'] - df_eng['MinTemp']\n",
        "\n",
        "# Average temperature for the day\n",
        "df_eng['AvgTemp'] = (df_eng['MinTemp'] + df_eng['MaxTemp']) / 2\n",
        "\n",
        "print(\"Created temperature-based features: TempRange, AvgTemp\")"
      ],
      "metadata": {
        "id": "ZcTZwmByax89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eng['PressureChange'] = df_eng['Pressure3pm'] - df_eng['Pressure9am']\n",
        "\n",
        "print(\"Created pressure-based feature: PressureChange\")"
      ],
      "metadata": {
        "id": "ELxHwih2ax6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eng['AvgWindSpeed'] = (df_eng['WindSpeed9am'] + df_eng['WindSpeed3pm']) / 2\n",
        "\n",
        "print(\"Created wind-based feature: AvgWindSpeed\")"
      ],
      "metadata": {
        "id": "Muw00XN-ax3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eng['HumidityTemp_Interaction'] = df_eng['AvgTemp'] * (df_eng['Humidity3pm'] * 0.01)\n",
        "\n",
        "print(\"Created interaction feature: HumidityTemp_Interaction\")"
      ],
      "metadata": {
        "id": "AsrajQSnax0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eng = df_eng.drop('Date', axis=1)\n",
        "print(\"\\nDropped the original 'Date' column.\")"
      ],
      "metadata": {
        "id": "cTrN_C_4axyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  --- Display the results ---\n",
        "print(\"\\n--- DataFrame with New Features (first 5 rows) ---\")\n",
        "display(df_eng.head())\n",
        "\n",
        "print(f\"\\nOriginal number of columns: {len(weather_data.columns)}\")\n",
        "print(f\"New number of columns: {len(df_eng.columns)}\")"
      ],
      "metadata": {
        "id": "at3pUGrbaxvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Preparing 'RainToday' Feature ---\")\n",
        "if 'RainToday' in df_eng.columns and df_eng['RainToday'].dtype == 'object':\n",
        "    df_eng['RainToday'] = df_eng['RainToday'].map({'No': 0, 'Yes': 1})\n",
        "    print(\"Converted 'RainToday' to 0s and 1s. It will be treated as CATEGORICAL in the pipeline.\")\n",
        "\n",
        "print(\"\\n--- Final DataFrame Info ---\")\n",
        "df_eng.info()"
      ],
      "metadata": {
        "id": "XH-alsmyaxsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Test Split**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Sr2qyBrifIC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Splitting the data based on the 'Year' column ---\n",
        "train_df = df_eng[df_eng.Year < 2015].copy()\n",
        "val_df = df_eng[df_eng.Year == 2015].copy()\n",
        "test_df = df_eng[df_eng.Year > 2015].copy()\n",
        "\n",
        "# --- Verify the Shapes ---\n",
        "print(\"--- DataFrame Shapes after Time-Based Splitting ---\")\n",
        "print(\"Training DataFrame shape:\", train_df.shape)\n",
        "print(\"Validation DataFrame shape:\", val_df.shape)\n",
        "print(\"Test DataFrame shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "X_HV2WO7axpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "Gs3ylCx9axm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df"
      ],
      "metadata": {
        "id": "Vq9RxgXoaxjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "Z7Xom66Raxg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input and Target Columns**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2REGteSrfUCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'RainTomorrow'\n",
        "\n",
        "# Create Training sets\n",
        "X_train = train_df.drop(target_column, axis=1)\n",
        "y_train = train_df[target_column]"
      ],
      "metadata": {
        "id": "5vb92cQ0axeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Validation sets\n",
        "X_val = val_df.drop(target_column, axis=1)\n",
        "y_val = val_df[target_column]"
      ],
      "metadata": {
        "id": "gTCa-ibBaxbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Test sets\n",
        "X_test = test_df.drop(target_column, axis=1)\n",
        "y_test = test_df[target_column]"
      ],
      "metadata": {
        "id": "dTW6Q39naxW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Verify the Shapes ---\n",
        "print(\"--- Final Dataset Shapes ---\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Shape of X_val:   {X_val.shape}\")\n",
        "print(f\"Shape of y_val:   {y_val.shape}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Shape of X_test:  {X_test.shape}\")\n",
        "print(f\"Shape of y_test:  {y_test.shape}\")"
      ],
      "metadata": {
        "id": "m3Gl46oFaxVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CatBoost**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DeAjMt95gbg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d-5ly12Wfin8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Imports ---\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# --- 2. Define Feature Lists ---\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "if 'RainToday' in numerical_features:\n",
        "    numerical_features.remove('RainToday')\n",
        "    categorical_features.append('RainToday')\n",
        "\n",
        "# --- 3. Define the Data Type Conversion Function ---\n",
        "# This function will be applied to our categorical features.\n",
        "def to_string(df):\n",
        "    return df.astype(str)\n",
        "\n",
        "# --- 4. Create the Definitive Preprocessing Pipeline ---\n",
        "# This version includes the crucial data type conversion step.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numerical_features),\n",
        "\n",
        "        ('cat', Pipeline(steps=[\n",
        "            # STEP 1: Impute missing values first.\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
        "            # STEP 2: Convert all values in these columns to strings.\n",
        "            ('caster', FunctionTransformer(to_string))\n",
        "        ]), categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# --- 5. Get Categorical Feature Indices ---\n",
        "# This logic remains the same and is correct.\n",
        "cat_feature_indices = list(range(len(numerical_features), len(numerical_features) + len(categorical_features)))\n",
        "\n",
        "print(f\"Numerical features count: {len(numerical_features)}\")\n",
        "print(f\"Categorical features count: {len(categorical_features)}\")\n",
        "print(f\"CatBoost will receive categorical features at indices: {cat_feature_indices}\")\n",
        "\n",
        "# --- 6. Create the Final CatBoost Pipeline ---\n",
        "catboost_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', CatBoostClassifier(\n",
        "        cat_features=cat_feature_indices,\n",
        "        task_type='GPU',\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"\\n‚úÖ Final, corrected CatBoost pipeline created successfully.\")\n",
        "display(catboost_pipeline)"
      ],
      "metadata": {
        "id": "X--70Ahtfg9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation and Selection**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-MDEAGvphXk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the cat pipeline...\")\n",
        "catboost_pipeline.fit(X_train, y_train)\n",
        "print(\"‚úÖ Training complete.\")"
      ],
      "metadata": {
        "id": "Y_RhmI8_fg4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
        "\n",
        "print(\"\\nMaking predictions on the validation data...\")\n",
        "val_preds = catboost_pipeline.predict(X_val)\n",
        "val_preds_proba = catboost_pipeline.predict_proba(X_val)[:, 1] # Get probabilities for AUC-ROC\n",
        "\n",
        "print(\"\\n--- Baseline catboost Evaluation ---\")\n",
        "f1 = f1_score(y_val, val_preds)\n",
        "roc_auc = roc_auc_score(y_val, val_preds_proba)\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC:  {roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_val, val_preds))"
      ],
      "metadata": {
        "id": "pVE8hgi-fg2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving through Joblib**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k-gnOT4VhiIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_filename = '09_catboost_champion.joblib'\n",
        "\n",
        "print(f\"--- üíæ Saving model to '{model_filename}' in the local session ---\")\n",
        "joblib.dump(catboost_pipeline, model_filename)\n",
        "\n",
        "print(f\"\\n‚úÖ Success! The champion CatBoost model has been saved locally.\")"
      ],
      "metadata": {
        "id": "WzGoqFU-fgzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xHzn0t0Shq_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w7JpPaKi0zZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Identify Numerical and Categorical Columns from X_train\n",
        "# -----------------------------------------------------------\n",
        "# Note: We explicitly drop 'Year' as it was only for splitting and we don't want it in the model.\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns.drop('Year').tolist()\n",
        "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# The 'RainToday' column is currently a float (0.0/1.0), but it's conceptually categorical.\n",
        "# Let's move it to the categorical list to be one-hot encoded.\n",
        "if 'RainToday' in numerical_features:\n",
        "    numerical_features.remove('RainToday')\n",
        "    categorical_features.append('RainToday')\n",
        "\n",
        "print(f\"Identified {len(numerical_features)} numerical features for the pipeline.\")\n",
        "print(f\"Identified {len(categorical_features)} categorical features for the pipeline.\")\n",
        "\n",
        "# 2. Construct the Preprocessing Pipelines\n",
        "# ----------------------------------------\n",
        "# Numerical pipeline: Median imputation + Standard scaling\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: Missing value imputation + One-hot encoding\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent',fill_value='unknown')), # Using most_frequent is a safe bet for categoricals\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n",
        "])\n",
        "\n",
        "# 3. Combine Preprocessing Steps with ColumnTransformer\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified (like 'Year')\n",
        ")\n",
        "\n",
        "# 4. Create the Full Model Pipeline\n",
        "# ---------------------------------\n",
        "# This chains the preprocessor and the classifier together.\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42)) # liblinear is good for this dataset size\n",
        "])\n",
        "\n",
        "print(\"\\n‚úÖ Preprocessing and full model pipelines created successfully!\")\n",
        "\n",
        "model_pipeline"
      ],
      "metadata": {
        "id": "ocptqiSjfgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GridSearch CV**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NrvPqHJa1Apr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Assume 'preprocessor', 'X_train', and 'y_train' are already defined.\n",
        "\n",
        "# --- 1. Define the Model Pipeline ---\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42))\n",
        "])\n",
        "\n",
        "# --- 2. Define the Hyperparameter Grid to Search ---\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10, 100],\n",
        "    'classifier__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# --- 3. Set up and Run GridSearchCV ---\n",
        "print(\"--- Running GridSearchCV for Logistic Regression ---\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# This is the main training and tuning step.\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# --- 4. Display the Results ---\n",
        "print(\"\\n‚úÖ GridSearchCV Complete.\")\n",
        "print(f\"Best F1-Score found during cross-validation: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Best Hyperparameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# You can also view the detailed results for all combinations\n",
        "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "print(\"\\n--- Detailed CV Results (Top 5) ---\")\n",
        "display(cv_results_df[['param_classifier__C', 'param_classifier__class_weight', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score').head())\n",
        "\n",
        "# The 'grid_search' object itself is now the best version of the model,\n",
        "# retrained on all the training data, ready for evaluation or saving.\n",
        "best_lr_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "60Tn-wn0fgsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation and Selection**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bzJ4VgmY12Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "est_lr_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"‚úÖ Best model extracted from GridSearchCV.\")\n",
        "print(f\"The best model has parameters: C=0.1 and class_weight=None\")"
      ],
      "metadata": {
        "id": "3plSCn8InEwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# --- Step 1: Get the best model from the completed Grid Search ---\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "print(\"‚úÖ Best model extracted from GridSearchCV.\")\n",
        "\n",
        "# --- Step 2: Make predictions on the validation set ---\n",
        "print(\"Making predictions on the unseen validation data...\")\n",
        "val_preds = best_lr_model.predict(X_val)\n",
        "val_preds_proba = best_lr_model.predict_proba(X_val)[:, 1] # Get probabilities for the 'Yes' class for AUC\n",
        "\n",
        "# --- Step 3: Calculate and print the final scores ---\n",
        "final_f1_score = f1_score(y_val, val_preds)\n",
        "final_auc_score = roc_auc_score(y_val, val_preds_proba)\n",
        "\n",
        "print(\"\\n--- Official Gauntlet Score for Logistic Regression ---\")\n",
        "print(f\"Validation F1-Score: {final_f1_score:.4f}\")\n",
        "print(f\"Validation AUC-ROC:  {final_auc_score:.4f}\")\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"\\nThis is the score to beat for all future models.\")"
      ],
      "metadata": {
        "id": "Fu_l4jI3nErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving through Joblib**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sEQ0PzaT2d2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "correct_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', C=10))\n",
        "])\n",
        "\n",
        "correct_model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Corrected model trained successfully.\")\n",
        "\n",
        "\n",
        "model_filename = '01_logistic_regression_tuned.joblib'\n",
        "joblib.dump(correct_model, model_filename)\n",
        "print(f\"\\n‚úÖ Better model has been saved to '{model_filename}'.\")\n",
        "print(\"The old model file has been overwritten.\")"
      ],
      "metadata": {
        "id": "xUaXj6r9nEpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ARX-VLfg5E2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell installs RAPIDS (which includes cuML).\n",
        "# The installation process will take approximately 10-15 minutes.\n",
        "#\n",
        "# IMPORTANT: The session will automatically restart after the installation is\n",
        "# complete. This is a normal and required part of the process.\n",
        "\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ],
      "metadata": {
        "id": "NN47C-Lq5LsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1-VA7iWH5ZD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from cuml.ensemble import RandomForestClassifier as cuMLRandomForest\n",
        "\n",
        "# We assume 'X_train' is your training DataFrame, ready to be used.\n",
        "\n",
        "# --- 1. Identify Numerical and Categorical Columns ---\n",
        "# This process remains consistent.\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# Move 'RainToday' to the categorical list\n",
        "if 'RainToday' in numerical_features:\n",
        "    numerical_features.remove('RainToday')\n",
        "    categorical_features.append('RainToday')\n",
        "\n",
        "# Create the final list of numerical columns to be processed, excluding 'Year'.\n",
        "final_numerical_features = [col for col in numerical_features if col != 'Year']\n",
        "\n",
        "print(f\"Identified {len(final_numerical_features)} numerical features for the pipeline.\")\n",
        "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
        "\n",
        "\n",
        "# --- 2. Define the Preprocessing Steps (Our Standard Preprocessor) ---\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()) # Kept for consistency\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "\n",
        "# --- 3. Create the Master Preprocessor ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, final_numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "\n",
        "# --- 4. Create the Final Random Forest Pipeline ---\n",
        "\n",
        "rf_gpu_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', cuMLRandomForest(random_state=42)) # Use the cuML version\n",
        "])\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ Full Random Forest pipeline created successfully.\")\n",
        "print(\"\\nPipeline Steps:\")\n",
        "display(rf_gpu_pipeline)"
      ],
      "metadata": {
        "id": "4LAzHpXk5LiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomizedSearch CV**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eA8dqyI95kJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf\n",
        "from cuml.ensemble import RandomForestClassifier as cuMLRandomForest\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import randint\n",
        "\n",
        "# --- 2. Create the Final GPU Pipeline ---\n",
        "\n",
        "rf_gpu_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', cuMLRandomForest(random_state=42))\n",
        "])\n",
        "\n",
        "# --- 3. Define the Hyperparameter Grid ---\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': randint(100, 500),\n",
        "    'classifier__max_depth': randint(10, 25),\n",
        "    'classifier__min_samples_split': randint(2, 20),\n",
        "    'classifier__min_samples_leaf': randint(1, 20),\n",
        "    'classifier__max_features': ['sqrt', 'log2', 0.5, 0.7]\n",
        "}\n",
        "\n",
        "# --- 4. Set up and Run RandomizedSearchCV ---\n",
        "\n",
        "random_search_gpu = RandomizedSearchCV(\n",
        "    estimator=rf_gpu_pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=2,\n",
        "    scoring='f1',\n",
        "    refit=True,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting FINAL GPU-Accelerated Randomized Search...\")\n",
        "# We fit on the original pandas DataFrames. The pipeline handles the rest.\n",
        "random_search_gpu.fit(X_train, y_train)\n",
        "print(\"\\n‚úÖ FINAL GPU-Accelerated Randomized Search Complete.\")\n",
        "\n",
        "# --- 5. Display Results ---\n",
        "print(\"\\nüèÜ Best Hyperparameters Found:\")\n",
        "print(random_search_gpu.best_params_)\n",
        "print(\"\\nBest F1-Score from Cross-Validation:\")\n",
        "print(f\"{random_search_gpu.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "u7E4fEtx5LOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation and Selection**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dcRFE9T15n6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Imports ---\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# --- 2. Extract the Best Model ---\n",
        "best_rf_model = random_search_gpu.best_estimator_\n",
        "print(\"‚úÖ Best Random Forest model extracted from RandomizedSearchCV.\")\n",
        "print(f\"The model has parameters: {random_search_gpu.best_params_}\")\n",
        "\n",
        "\n",
        "# --- 3. Make Predictions on the Validation Set ---\n",
        "print(\"\\nMaking predictions on the unseen validation data...\")\n",
        "\n",
        "# ALWAYS pass a pandas DataFrame to the start of an sklearn pipeline.\n",
        "# The pipeline will handle the CPU-to-GPU data transfer internally.\n",
        "val_preds = best_rf_model.predict(X_val)\n",
        "val_preds_proba = best_rf_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "\n",
        "# --- 4. Calculate and Print the Final Scores ---\n",
        "final_f1_score = f1_score(y_val, val_preds)\n",
        "final_auc_score = roc_auc_score(y_val, val_preds_proba)\n",
        "\n",
        "print(\"\\n--- üèÜ Official Gauntlet Score for Tuned Random Forest üèÜ ---\")\n",
        "print(f\"Validation F1-Score: {final_f1_score:.4f}\")\n",
        "print(f\"Validation AUC-ROC:  {final_auc_score:.4f}\")\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# --- 5. Display the Full Classification Report ---\n",
        "print(\"\\n--- Full Classification Report ---\")\n",
        "report = classification_report(y_val, val_preds, target_names=['No Rain', 'Rain'])\n",
        "print(report)\n",
        "\n",
        "\n",
        "# --- 6. Display the Confusion Matrix using Plotly ---\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "conf_matrix = confusion_matrix(y_val, val_preds)\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix,\n",
        "    x=['Predicted No Rain', 'Predicted Rain'],\n",
        "    y=['Actual No Rain', 'Actual Rain'],\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "fig.update_layout(title_text='Confusion Matrix (Tuned Random Forest)')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "xa7fOmPWnEkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving through Joblib**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vqtzXe4n5yQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_filename = '05_random_forest_tuned.joblib'\n",
        "\n",
        "best_rf_model = random_search_gpu.best_estimator_\n",
        "\n",
        "joblib.dump(best_rf_model, model_filename)\n",
        "\n",
        "print(f\"‚úÖ Model successfully saved to the temporary session storage as '{model_filename}'.\")"
      ],
      "metadata": {
        "id": "wlGg_iP1axQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNTM5Y0qaxOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}